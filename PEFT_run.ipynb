{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Oğuz Kağan Hitit, Koç University, 2024",
   "metadata": {
    "id": "4SQfHe-XQk0l"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Installing the dependencies",
   "metadata": {
    "id": "ePBIBrayQZih"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets\n",
    "!pip install git+https://github.com/huggingface/peft\n",
    "!pip install wandb"
   ],
   "metadata": {
    "id": "vUcEap0JTYwL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating the model\n"
   ],
   "metadata": {
    "id": "MZmFNsMgQXf0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "class SentimentModel:\n",
    "    def __init__(self, model_name='gpt2-medium-peft-lora', config=None):\n",
    "        self.model_name = model_name\n",
    "        self.model_dir = '/content/drive/MyDrive/comp542_assignment3/lora-gpt-peft'\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.config = GPT2Config.from_pretrained('gpt2', **config)\n",
    "        base_model = GPT2LMHeadModel.from_pretrained('gpt2', config=self.config)\n",
    "        print('Using GPT2-medium...')\n",
    "        lora_config = LoraConfig(r=config['lora_r'], lora_alpha=config['lora_alpha'],\n",
    "                                 lora_dropout=config['dropout'], task_type=\"CAUSAL_LM\")\n",
    "        print('lora_r:', config['lora_r'])\n",
    "        print('alpha:', config['lora_alpha'])\n",
    "        print('dropout:', config['dropout'])\n",
    "        print('lr:', config['learning_rate'])\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "        self.model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "        wandb.init(project=\"lora_finetune\", entity=\"ohitit20\", config=config)\n",
    "        wandb.watch(self.model, log='all')\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "        train_dataset = dataset[\"train\"].map(self.add_instruction_finetuning)\n",
    "        train_dataset = train_dataset.map(self.preprocess_data, batched=True)\n",
    "        self.train_loader = DataLoader(train_dataset.with_format(\"torch\"), batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "        val_dataset = dataset['validation'].map(self.add_instruction_finetuning)\n",
    "        val_dataset = val_dataset.map(self.preprocess_data, batched=True)\n",
    "        self.val_loader = DataLoader(val_dataset.with_format('torch'), batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "        self.test_dataset = dataset['test']\n",
    "        print(f'Dataset loaded with {len(self.train_loader.dataset)} training samples.')\n",
    "\n",
    "    def add_instruction_finetuning(self, rec):\n",
    "        instruction = \"Label the sentiment of the following sentence\"\n",
    "        INSTRUCTION_TEMPLATE = \"{}\\nSentence:{}\\nLabel:{}\"\n",
    "        label = 'positive' if rec['label'] == 1 else 'negative'\n",
    "        rec[\"instr_tuned_text\"] = INSTRUCTION_TEMPLATE.format(instruction, rec['text'], label)\n",
    "        return rec\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        return self.tokenizer(data['instr_tuned_text'], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    def train(self, epochs):\n",
    "      print('Training loop:')\n",
    "      self.model.train()\n",
    "      for epoch in range(epochs):\n",
    "          self.optimizer.zero_grad()\n",
    "          total_loss = 0\n",
    "          total_val_loss = 0\n",
    "\n",
    "          for step, batch in enumerate(tqdm(self.train_loader, desc=f'Epoch {epoch+1}')):\n",
    "              inputs = {k: v.to(self.model.device) for k, v in batch.items() if k in self.tokenizer.model_input_names}\n",
    "              outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
    "              loss = outputs.loss\n",
    "              loss = loss / config['gradient_accumulation_steps']\n",
    "              loss.backward()\n",
    "\n",
    "              if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                  self.optimizer.step()\n",
    "                  self.optimizer.zero_grad()\n",
    "\n",
    "              total_loss += loss.item()\n",
    "\n",
    "              if (step + 1) % 50 == 0:\n",
    "                  print(f'Batch {step+1}, Training Loss: {loss.item()}')\n",
    "\n",
    "          self.model.eval()\n",
    "          with torch.no_grad():\n",
    "              for val_batch in self.val_loader:\n",
    "                  val_inputs = {k: v.to(self.model.device) for k, v in val_batch.items() if k in self.tokenizer.model_input_names}\n",
    "                  val_outputs = self.model(**val_inputs, labels=val_inputs[\"input_ids\"])\n",
    "                  val_loss = val_outputs.loss\n",
    "                  total_val_loss += val_loss.item()\n",
    "\n",
    "          avg_train_loss = total_loss / len(self.train_loader)\n",
    "          avg_val_loss = total_val_loss / len(self.val_loader)\n",
    "          wandb.log({\"epoch\": epoch + 1, \"training_loss\": avg_train_loss, \"validation_loss\": avg_val_loss,\n",
    "                       \"learning_rate\": config['learning_rate'], \"lora_r\": config['lora_r'], \"lora_alpha\": config['lora_alpha']})\n",
    "          print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "          torch.save(self.model.state_dict(), f\"{self.model_dir}/{self.model_name}_epoch{epoch+1}.pt\")\n",
    "          self.model.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "      predictions = []\n",
    "      targets = []\n",
    "      positive_predictions = 0\n",
    "      negative_predictions = 0\n",
    "      counter = 0\n",
    "\n",
    "      instruction = 'Label the sentiment of following sentence'\n",
    "      INSTRUCTION_TEMPLATE = \"{}:\\nSentence:{}\\nLabel:\"\n",
    "      for example in self.test_dataset:\n",
    "          prompt_text = INSTRUCTION_TEMPLATE.format(instruction, example['text'])\n",
    "          target_label = 'positive' if example['label'] == 1 else 'negative'\n",
    "          targets.append(target_label)\n",
    "\n",
    "          generated_text = self.generate_text(prompt_text)\n",
    "          if('<|endoftext|>' in generated_text):\n",
    "              generated_text = generated_text.split('<|endoftext|>')[0]\n",
    "          positive = 'positive' in generated_text\n",
    "          negative = 'negative' in generated_text\n",
    "\n",
    "          if positive and negative:\n",
    "              predicted_label = 'null'\n",
    "          elif positive:\n",
    "              predicted_label = 'positive'\n",
    "              positive_predictions += 1\n",
    "          elif negative:\n",
    "              predicted_label = 'negative'\n",
    "              negative_predictions += 1\n",
    "          else:\n",
    "              predicted_label = 'null'\n",
    "          predictions.append(predicted_label)\n",
    "          if(counter % 5 == 2):\n",
    "              print('Prompt text:' + prompt_text)\n",
    "              print('Generated text:' + generated_text)\n",
    "              print('Target label:' + target_label)\n",
    "              print('Predicted label:' + predicted_label)\n",
    "          counter += 1\n",
    "\n",
    "      accuracy = sum([1 for pred, target in zip(predictions, targets) if pred == target]) / len(targets)\n",
    "      precision = precision_score(targets, predictions, labels=[\"positive\", \"negative\"], average='micro')\n",
    "      recall = recall_score(targets, predictions, labels=[\"positive\", \"negative\"], average='micro')\n",
    "      f1 = f1_score(targets, predictions, labels=[\"positive\", \"negative\"], average='micro')\n",
    "      print('Accuracy:', accuracy, 'Precision:', precision, 'Recall:', recall, 'F1:', f1)\n",
    "      return accuracy, positive_predictions, negative_predictions\n",
    "\n",
    "    def generate_text(self, input_text):\n",
    "        inputs = self.tokenizer.encode(input_text, return_tensors='pt')\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(inputs, max_length=512, num_return_sequences=1)\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text"
   ],
   "metadata": {
    "id": "Qbr2YiVzIJQS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training and evaluation\n",
   "metadata": {
    "id": "ZYqTh7IXQe6M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"dropout\": 0.05,\n",
    "    \"max_iters\": 50,\n",
    "    \"gradient_accumulation_steps\": 32,\n",
    "}\n",
    "\n",
    "model = SentimentModel(config=config)\n",
    "model.prepare_data()\n",
    "model.train(epochs=config['epochs'])\n",
    "model.evaluate()\n"
   ],
   "metadata": {
    "id": "SUDCM6G_JymU"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
